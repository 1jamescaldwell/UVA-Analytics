{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6d4a172",
   "metadata": {},
   "source": [
    "James Caldwell <br>\n",
    "UVA IRA, 10/8/2025 <br>\n",
    "\n",
    "This Python script automates the extraction of error tables from the SCHEV Institutional Portal and merges them with a VCSIN-to-SSID mapping file. The final result is exported to an Excel workbook with each error code on a separate sheet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ebc74f",
   "metadata": {},
   "source": [
    "Features: <br>\n",
    "Opens the SCHEV portal in a Chrome browser via Selenium.\n",
    "\n",
    "Allows manual login for secure access.\n",
    "\n",
    "Extracts links corresponding to error codes.\n",
    "\n",
    "Follows each link to retrieve error tables.\n",
    "\n",
    "Merges error tables with a local VCSIN-to-SSID Excel mapping.\n",
    "\n",
    "Saves all merged tables to a single Excel workbook with meaningful sheet names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cf11c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from urllib.parse import urljoin\n",
    "from io import StringIO\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# to do: summary page error descriptions for completions and assigned to isn't working?\n",
    "\n",
    "# Set working directory\n",
    "os.chdir(\"X:/SCHEV/2425 (2024-2025)/Error & Warning Reports/Download and Iteration Check scripts\")\n",
    "\n",
    "# Variables to change\n",
    "url = \"https://portals.schev.edu/institutions/PUBLIC/UVA/Viewerrorsummary.asp?tablename=fa&repyear=2425\"\n",
    "load_dotenv()\n",
    "VCSIN_to_SSID_path = os.getenv('VCSIN_to_SSID_path') \n",
    "chrome_driver_path = os.getenv('chrome_driver_path') # This won't run on the V: drive. Put in folder on personal computer. Download from: https://googlechromelabs.github.io/chrome-for-testing/\n",
    "excel_output_path = os.getenv('excel_output_path') \n",
    "meta_data_path = os.getenv('meta_data_path')\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "options = Options()\n",
    "driver = webdriver.Chrome(options=options) \n",
    "\n",
    "# Open the login page\n",
    "driver.get(url)\n",
    "\n",
    "print(\"Browser opened. Please log in manually.\")\n",
    "version_number = input(\"Type input number and ENTER here *after* you have logged in successfully...\")\n",
    "\n",
    "# After you log in, grab the page\n",
    "driver.get(url)\n",
    "html = driver.page_source\n",
    "mysoup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Load VCSIN to SSID mapping file\n",
    "print(\"Loading VCSIN to SSID mapping file...\")\n",
    "VCSIN_to_SSID = pd.read_excel(VCSIN_to_SSID_path)\n",
    "\n",
    "# Load meta data file (This has error descriptions and links)\n",
    "print(\"Loading meta data file...\")\n",
    "meta_data = pd.read_excel(meta_data_path)\n",
    "\n",
    "# === Extract all links ===\n",
    "links = []\n",
    "for a in mysoup.find_all('a', href=True):\n",
    "    href = a['href']\n",
    "    # Skip empty, anchor, or javascript links\n",
    "    if href.startswith('#') or href.lower().startswith('javascript'):\n",
    "        continue\n",
    "    # Convert relative URLs to absolute\n",
    "    full_link = href if href.startswith('http') else driver.current_url.rsplit('/', 1)[0] + '/' + href\n",
    "    if 'viewError' in full_link:  # filter for relevant links\n",
    "        links.append(full_link)\n",
    "\n",
    "print(f\"\\nFound {len(links)} error code links:\")\n",
    "\n",
    "all_error_tables_for_excel = []\n",
    "all_error_table_names_for_excel = []\n",
    "# # === Visit each link ===\n",
    "for i, link in enumerate(links, start=1): # links[:3]\n",
    "    print(f\"\\nVisiting link {i}/{len(links)}: {link}\")\n",
    "    try:\n",
    "        driver.get(link)\n",
    "        time.sleep(1)  # wait for page to load; adjust as needed\n",
    "        \n",
    "        page_html = driver.page_source\n",
    "        page_soup = BeautifulSoup(page_html, 'html.parser')\n",
    "\n",
    "        # Find the <script> tag containing the specific substring for error table url\n",
    "        link_tag = page_soup.find(src=lambda value: value and \"ErrorsList.asp\" in value)\n",
    "        if link_tag:\n",
    "            \n",
    "            ## save Error Name for excel sheet name\n",
    "            match = re.search(r'Errcode=([^&\"]+)', page_html)\n",
    "            if match:\n",
    "                error_code = match.group(1)\n",
    "                print(f'Error Code: {error_code}')\n",
    "                all_error_table_names_for_excel.append(error_code)  # error_code = match.group(1)\n",
    "\n",
    "            ## Follow error table URL and save\n",
    "            relative_url = link_tag['src']\n",
    "            # print(\"Relative URL:\", relative_url)\n",
    "\n",
    "            # Convert to absolute\n",
    "            base = \"https://portals.schev.edu/institutions/PUBLIC/UVA/\"\n",
    "            table_url = urljoin(base, relative_url)\n",
    "            # print(\"Full URL:\", table_url)\n",
    "\n",
    "            driver.get(table_url)\n",
    "            time.sleep(1) # wait for page to load; adjust as needed\n",
    "            table_page_html = driver.page_source\n",
    "\n",
    "            # Parse tables with pandas\n",
    "            tables = pd.read_html(StringIO(table_page_html), header=0)\n",
    "            df = tables[0]  # Get the first table, adjust index if needed\n",
    "\n",
    "            # sometimes it's socsec1, sometimes SOCSEC1. Capitalize it if needed.\n",
    "            df.columns = [str(c).upper() if str(c).lower() == 'socsec1' else c for c in df.columns]\n",
    "\n",
    "            if 'SOCSEC1' in df.columns:\n",
    "                df = df.merge(\n",
    "                    VCSIN_to_SSID,\n",
    "                    how='left',\n",
    "                    left_on='SOCSEC1',\n",
    "                    right_on='VCSIN'\n",
    "                ).drop(columns='VCSIN')\n",
    "\n",
    "            df[f'Comments V{version_number}'] = ''\n",
    "            all_error_tables_for_excel.append(df)\n",
    "\n",
    "        else:\n",
    "            print(\"No match found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error visiting {link}: {e}\")\n",
    "\n",
    "print(\"\\nDone visiting all links.\")\n",
    "\n",
    "print(\"\\nSaving to Excel...\")\n",
    "with pd.ExcelWriter(excel_output_path, engine='openpyxl') as writer:\n",
    "    for i, df in enumerate(all_error_tables_for_excel):\n",
    "        sheet_name = all_error_table_names_for_excel[i]\n",
    "        \n",
    "        # Write each DataFrame to a separate sheet\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False,startrow=2)\n",
    "        \n",
    "        # Put description and link in the first two rows\n",
    "        try:\n",
    "            description = meta_data.loc[meta_data['ErrCode'] == sheet_name]['Description'].values[0]\n",
    "            link = meta_data.loc[meta_data['ErrCode'] == sheet_name]['Link'].values[0]\n",
    "            worksheet = writer.sheets[sheet_name]\n",
    "            worksheet.cell(row=1, column=1).value = f\"Description: {description}\"\n",
    "            worksheet.cell(row=2, column=3).value = f\"Link: {link}\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding metadata for sheet {sheet_name}: {e}\")\n",
    "            # This section will probably not fail this year (2025), but could fail if new errors are seen next year that aren't in the meta data file. Add them and re-run.\n",
    "print(\"\\nDone.\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "769fac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to excel section if need to run again due to error \n",
    "# excel_output_path = r\"X:\\SCHEV\\2425 (2024-2025)\\Error & Warning Reports\\Download and Iteration Check scripts\\output.xlsx\"\n",
    "# print(\"\\nSaving to Excel...\")\n",
    "# with pd.ExcelWriter(excel_output_path, engine='openpyxl') as writer:\n",
    "#     for i, df in enumerate(all_error_tables_for_excel):\n",
    "#         sheet_name = all_error_table_names_for_excel[i]\n",
    "        \n",
    "#         # Write each DataFrame to a separate sheet\n",
    "#         df.to_excel(writer, sheet_name=sheet_name, index=False,startrow=2)\n",
    "        \n",
    "#         # Put description and link in the first two rows\n",
    "#         try:\n",
    "#             description = meta_data.loc[meta_data['ErrCode'] == sheet_name]['Description'].values[0]\n",
    "#             link = meta_data.loc[meta_data['ErrCode'] == sheet_name]['Link'].values[0]\n",
    "#             worksheet = writer.sheets[sheet_name]\n",
    "#             worksheet.cell(row=1, column=1).value = f\"Description: {description}\"\n",
    "#             worksheet.cell(row=2, column=2).value = f\"Link: {link}\"\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error adding metadata for sheet {sheet_name}: {e}\")\n",
    "#             # This section will probably not fail this year (2025), but could fail if new errors are seen next year that aren't in the meta data file. Add them and re-run."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6d4a172",
   "metadata": {},
   "source": [
    "James Caldwell <br>\n",
    "UVA IRA, 10/8/2025 <br>\n",
    "\n",
    "This Python script automates the extraction of error tables from the SCHEV Institutional Portal and merges them with a VCSIN-to-SSID mapping file. The final result is exported to an Excel workbook with each error code on a separate sheet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ebc74f",
   "metadata": {},
   "source": [
    "Features: <br>\n",
    "Opens the SCHEV portal in a Chrome browser via Selenium.\n",
    "\n",
    "Allows manual login for secure access.\n",
    "\n",
    "Extracts links corresponding to error codes.\n",
    "\n",
    "Follows each link to retrieve error tables.\n",
    "\n",
    "Merges error tables with a local VCSIN-to-SSID Excel mapping.\n",
    "\n",
    "Saves all merged tables to a single Excel workbook with meaningful sheet names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf9222c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b913520d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Browser opened. Please log in manually.\n"
     ]
    },
    {
     "ename": "InvalidSessionIdException",
     "evalue": "Message: invalid session id: session deleted as the browser has closed the connection\nfrom disconnected: not connected to DevTools\n  (Session info: chrome=143.0.7499.111)\nStacktrace:\nSymbols not available. Dumping unresolved backtrace:\n\t0x7ff65cb88895\n\t0x7ff65cb888f0\n\t0x7ff65c96165d\n\t0x7ff65c94d202\n\t0x7ff65c9727af\n\t0x7ff65c9e9a29\n\t0x7ff65ca0a5c2\n\t0x7ff65c9aac29\n\t0x7ff65c9aba93\n\t0x7ff65cea05f0\n\t0x7ff65ce9af30\n\t0x7ff65ceb9696\n\t0x7ff65cba5d94\n\t0x7ff65cbaed3c\n\t0x7ff65cb91fb4\n\t0x7ff65cb92165\n\t0x7ff65cb77e92\n\t0x7ffae737e8d7\n\t0x7ffae87cc53c\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidSessionIdException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m version_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType input number and ENTER here *after* you have logged in successfully...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# After you log in, grab the page\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m html \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mpage_source\n\u001b[0;32m     40\u001b[0m mysoup \u001b[38;5;241m=\u001b[39m BeautifulSoup(html, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ywe4kw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:454\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    437\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Navigate the browser to the specified URL in the current window or\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;124;03m    tab.\u001b[39;00m\n\u001b[0;32m    439\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;124;03m    >>> driver.get(\"https://example.com\")\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 454\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ywe4kw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:429\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    427\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 429\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    430\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\ywe4kw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:232\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    230\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mInvalidSessionIdException\u001b[0m: Message: invalid session id: session deleted as the browser has closed the connection\nfrom disconnected: not connected to DevTools\n  (Session info: chrome=143.0.7499.111)\nStacktrace:\nSymbols not available. Dumping unresolved backtrace:\n\t0x7ff65cb88895\n\t0x7ff65cb888f0\n\t0x7ff65c96165d\n\t0x7ff65c94d202\n\t0x7ff65c9727af\n\t0x7ff65c9e9a29\n\t0x7ff65ca0a5c2\n\t0x7ff65c9aac29\n\t0x7ff65c9aba93\n\t0x7ff65cea05f0\n\t0x7ff65ce9af30\n\t0x7ff65ceb9696\n\t0x7ff65cba5d94\n\t0x7ff65cbaed3c\n\t0x7ff65cb91fb4\n\t0x7ff65cb92165\n\t0x7ff65cb77e92\n\t0x7ffae737e8d7\n\t0x7ffae87cc53c\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from urllib.parse import urljoin\n",
    "from io import StringIO\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Set working directory\n",
    "    # Uncomment to set as needed\n",
    "# os.chdir(\"path...\\Error & Warning Reports\\Download and Iteration Check scripts\")\n",
    "\n",
    "# Variables to change\n",
    "url = \"https://portals.schev.edu/institutions/PUBLIC/UVA/Viewerrorsummary.asp?tablename=fa&repyear=2425\"\n",
    "load_dotenv()\n",
    "VCSIN_to_SSID_path = os.getenv('VCSIN_to_SSID_path') \n",
    "chrome_driver_path = os.getenv('chrome_driver_path') # This won't run on the V: drive. Put in folder on personal computer. Download from: https://googlechromelabs.github.io/chrome-for-testing/\n",
    "excel_output_path = os.getenv('excel_output_path') \n",
    "meta_data_path = os.getenv('meta_data_path')\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "options = Options()\n",
    "driver = webdriver.Chrome(options=options) \n",
    "\n",
    "# Open the login page\n",
    "driver.get(url)\n",
    "\n",
    "print(\"Browser opened. Please log in manually.\")\n",
    "version_number = input(\"Type input number and ENTER here *after* you have logged in successfully...\")\n",
    "\n",
    "# After you log in, grab the page\n",
    "driver.get(url)\n",
    "html = driver.page_source\n",
    "mysoup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Load VCSIN to SSID mapping file\n",
    "print(\"Loading VCSIN to SSID mapping file...\")\n",
    "VCSIN_to_SSID = pd.read_excel(VCSIN_to_SSID_path)\n",
    "\n",
    "# Load meta data file (This has error descriptions and links)\n",
    "print(\"Loading meta data file...\")\n",
    "meta_data = pd.read_excel(meta_data_path)\n",
    "\n",
    "# === Extract all links ===\n",
    "links = []\n",
    "for a in mysoup.find_all('a', href=True):\n",
    "    href = a['href']\n",
    "    # Skip empty, anchor, or javascript links\n",
    "    if href.startswith('#') or href.lower().startswith('javascript'):\n",
    "        continue\n",
    "    # Convert relative URLs to absolute\n",
    "    full_link = href if href.startswith('http') else driver.current_url.rsplit('/', 1)[0] + '/' + href\n",
    "    if 'viewError' in full_link:  # filter for relevant links\n",
    "        links.append(full_link)\n",
    "\n",
    "print(f\"\\nFound {len(links)} error code links:\")\n",
    "\n",
    "all_error_tables_for_excel = []\n",
    "all_error_table_names_for_excel = []\n",
    "\n",
    "meta_check_df = pd.DataFrame({'url': links})\n",
    "meta_check_df['Err'] = meta_check_df['url'].str.extract(r'Err=([^&]+)')\n",
    "no_errors = meta_check_df['Err'].isin(meta_data['ErrCode']).all() # True if all error codes in links are in meta_data, otherwise False\n",
    "if no_errors: # Check that all error codes in links are in meta_data. \n",
    "    print(\"All error codes found in meta_data.\")\n",
    "    # # === Visit each link ===\n",
    "    for i, link in enumerate(links, start=1): # links[:3]\n",
    "        print(f\"\\nVisiting link {i}/{len(links)}: {link}\")\n",
    "        try:\n",
    "            driver.get(link)\n",
    "            time.sleep(1)  # wait for page to load; adjust as needed\n",
    "            \n",
    "            page_html = driver.page_source\n",
    "            page_soup = BeautifulSoup(page_html, 'html.parser')\n",
    "\n",
    "            # Find the <script> tag containing the specific substring for error table url\n",
    "            link_tag = page_soup.find(src=lambda value: value and \"ErrorsList.asp\" in value)\n",
    "            if link_tag:\n",
    "                \n",
    "                ## save Error Name for excel sheet name\n",
    "                match = re.search(r'Errcode=([^&\"]+)', page_html)\n",
    "                if match:\n",
    "                    error_code = match.group(1)\n",
    "                    print(f'Error Code: {error_code}')\n",
    "                    all_error_table_names_for_excel.append(error_code)  # error_code = match.group(1)\n",
    "\n",
    "                ## Follow error table URL and save\n",
    "                relative_url = link_tag['src']\n",
    "                # print(\"Relative URL:\", relative_url)\n",
    "\n",
    "                # Convert to absolute\n",
    "                base = \"https://portals.schev.edu/institutions/PUBLIC/UVA/\"\n",
    "                table_url = urljoin(base, relative_url)\n",
    "                # print(\"Full URL:\", table_url)\n",
    "\n",
    "                driver.get(table_url)\n",
    "                time.sleep(1) # wait for page to load; adjust as needed\n",
    "                table_page_html = driver.page_source\n",
    "\n",
    "                # Parse tables with pandas\n",
    "                tables = pd.read_html(StringIO(table_page_html), header=0)\n",
    "                df = tables[0]  # Get the first table, adjust index if needed\n",
    "\n",
    "                # sometimes it's socsec1, sometimes SOCSEC1. Capitalize it if needed.\n",
    "                df.columns = [str(c).upper() if str(c).lower() == 'socsec1' else c for c in df.columns]\n",
    "\n",
    "                if 'SOCSEC1' in df.columns:\n",
    "                    df = df.merge(\n",
    "                        VCSIN_to_SSID,\n",
    "                        how='left',\n",
    "                        left_on='SOCSEC1',\n",
    "                        right_on='VCSIN'\n",
    "                    ).drop(columns='VCSIN')\n",
    "\n",
    "                df[f'Comments V{version_number}'] = ''\n",
    "                all_error_tables_for_excel.append(df)\n",
    "\n",
    "            else:\n",
    "                print(\"No match found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error visiting {link}: {e}\")\n",
    "\n",
    "    print(\"\\nDone visiting all links.\")\n",
    "\n",
    "    print(\"\\nSaving to Excel...\")\n",
    "    with pd.ExcelWriter(excel_output_path, engine='openpyxl') as writer:\n",
    "        for i, df in enumerate(all_error_tables_for_excel):\n",
    "            sheet_name = all_error_table_names_for_excel[i]\n",
    "            \n",
    "            # Write each DataFrame to a separate sheet\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False,startrow=2)\n",
    "            \n",
    "            # Put description and link in the first two rows\n",
    "            try:\n",
    "                description = meta_data.loc[meta_data['ErrCode'] == sheet_name]['Description'].values[0]\n",
    "                link = meta_data.loc[meta_data['ErrCode'] == sheet_name]['Link'].values[0]\n",
    "                worksheet = writer.sheets[sheet_name]\n",
    "                worksheet.cell(row=1, column=1).value = f\"Description: {description}\"\n",
    "                worksheet.cell(row=2, column=3).value = f\"Link: {link}\"\n",
    "            except Exception as e:\n",
    "                print(f\"Error adding metadata for sheet {sheet_name}: {e}\")\n",
    "                # This section will probably not fail this year (2025), but could fail if new errors are seen next year that aren't in the meta data file. Add them and re-run.\n",
    "    print(\"\\nDone.\")\n",
    "    driver.quit()\n",
    "else:\n",
    "    # Find which new error codes are not in meta_data and send message to add them\n",
    "    for error in meta_check_df['Err']:\n",
    "        if error not in meta_data['ErrCode'].tolist():\n",
    "            print(f\"Error code {error} not found in meta_data. Please add and re-run\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
